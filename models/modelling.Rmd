```{r echo=F, warning=FALSE, error=FALSE}
opts_chunk$set(echo=TRUE, warning=FALSE, message=F, error=FALSE, textwidth=200)
mm <- read.csv("../measured_me.csv")
library(lubridate)
mm$Date <- parse_date_time(mm$Date,"md")
year(mm$Date) <- 2013
```

Modelling the Measured Me dataset
======================================

## Correlation Matrices

While bivariate comparisons are insightful, we must produce and inspect `r (ncol(mm)^2) - ncol(mm)` charts to cover the whole dataset. Instead lets measure the correlation systematically.

First we create a matrix, removing the `Date` column. Then we produce a correlation matrix, specifying that we only want to use those observations that are complete (i.e. ignoring those which have missing data). Without this specification we would see `NA` in the results.

```{r}
mm.matrix <- as.matrix(mm[,2:ncol(mm)])
cor(mm.matrix, use="complete.obs")
```

Although this is a good start, we can now see the Pearson correlation coefficient for each pair, it immediately raises the question: which are statistically significant? Enter the `Hmisc` library.

```{r}
library(Hmisc)
rcorr(mm.matrix)
```

This time we have three tables:

- Pearson's `r` (again)
- The number of observations used (note that `NA` pairs are removed, not rows)
- The `p` value of the coefficient

We're looking for `p` values lower than `0.05` to indicate statistical significance.

Searching through this table for significant correlations is quite tiresome. Let's use a visualisation to guide our eyes to the main relationships.

On the bottom left of the `correlogram` we see tiles coloured according to the strength (intensity) and direction (blue is positive, and red negative). On the top right we have concentration ellipses and loess smoothness curves.

```{r}
library(corrgram)
corrgram(mm.matrix, upper.panel=panel.ellipse)
```

We can rearrange the plot so that correlated variables are clustered together.

```{r}
corrgram(mm.matrix, order=T, upper.panel=panel.ellipse)
```

### Linear Modelling
Finally we can build a simple model to describe and predict, e.g. Stress.

First we remove the missing observations.
```{r}
mm.nona <- na.omit(mm)
```

Then create a simple linear model, regressing all other variables against Stress.
```{r}
stress.lm.basic <- lm(Stress ~ ., mm.nona)
summary(stress.lm.basic)
```

Then we can remove insignificant variables from the model through the stepwise algorithm.
```{r}
stress.lm.stepped <- step(stress.lm.basic)
summary(stress.lm.stepped)
```

Comparing the predict results with the actual.
```{r}
plot(fitted.values(stress.lm.stepped), mm.nona$Stress)
```

Comparing stress with the significant variables.
```{r fig.width=7, fig.height=14}
mm.nona.stress <- melt(mm.nona, id.vars="Stress")
mm.nona.stress.stepped <- mm.nona.stress[mm.nona.steps$variable %in% attr(terms(stress.lm.stepped),"term.labels"),]
ggplot(mm.nona.stress.stepped, aes(Stress, value)) + geom_hex(bins=10) + geom_smooth(method="lm", na.rm=T) + facet_wrap(~ variable, ncol=1, scales="free_y")
```

Some diagnostic tests on the model.
```{r}
plot(stress.lm.stepped)
```